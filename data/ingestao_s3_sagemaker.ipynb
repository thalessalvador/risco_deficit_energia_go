{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98e2d3-44bf-4318-8d80-4b905895c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando o teste de permissões do S3 ---\n",
      "Tentando listar os buckets...\n",
      "Buckets acessíveis: ['amazon-sagemaker-101859807606-us-east-2-29c5f0787bb5', 'labriscoenergeticomod2']\n",
      "\n",
      "Tentando escrever um arquivo de teste em s3://labriscoenergeticomod2/raw...\n",
      "\n",
      "✅ SUCESSO! O arquivo de teste 'raw/teste_acesso_d48d8e23-f594-4748-bfc0-3199bfb1311f.txt' foi criado com sucesso.\n",
      "Você pode verificar o arquivo no S3 em: https://s3.console.aws.amazon.com/s3/object/labriscoenergeticomod2?prefix=raw/teste_acesso_d48d8e23-f594-4748-bfc0-3199bfb1311f.txt\n",
      "\n",
      "Tentando ler o arquivo de teste...\n",
      "Conteúdo lido: 'Este é um arquivo de teste para verificar as permissões de escrita do SageMaker.'\n",
      "\n",
      "Tentando remover o arquivo de teste...\n",
      "✅ SUCESSO! O arquivo de teste foi removido.\n",
      "\n",
      "O ambiente do SageMaker tem as permissões de leitura, escrita e exclusão corretas para o bucket.\n"
     ]
    }
   ],
   "source": [
    "# TESTE DE CONEXÃO, DESCOMENTAR EM AMBIENTE SAGEMAKER\n",
    "# Este script testa se o ambiente do SageMaker tem as permissões corretas para acessar um bucket S3.\n",
    "# Ele tenta listar os buckets, escrever um arquivo de teste, ler esse arquivo e depois removê-lo.\n",
    "# Se todas as operações forem bem-sucedidas, o ambiente está corretamente configurado.\n",
    "# Caso contrário, mensagens de erro ajudarão a diagnosticar o problema.\n",
    "\n",
    "# import boto3\n",
    "# import uuid\n",
    "\n",
    "# # Substitua pelo nome do seu bucket\n",
    "# BUCKET_NAME = 'labriscoenergeticomod2'\n",
    "# # Defina o \"caminho\" (prefixo) da pasta que você deseja testar\n",
    "# FOLDER_PATH = 'raw'\n",
    "\n",
    "# # Cria um cliente S3\n",
    "# s3_client = boto3.client('s3')\n",
    "\n",
    "# print(\"--- Iniciando o teste de permissões do S3 ---\")\n",
    "\n",
    "# try:\n",
    "#     # 1. Tentar listar os buckets (verifica a permissão 's3:ListAllMyBuckets')\n",
    "#     print(\"Tentando listar os buckets...\")\n",
    "#     response = s3_client.list_buckets()\n",
    "#     buckets = [b['Name'] for b in response['Buckets']]\n",
    "#     print(f\"Buckets acessíveis: {buckets}\")\n",
    "\n",
    "#     # 2. Tentar escrever um arquivo de teste no bucket e pasta especificados\n",
    "#     print(f\"\\nTentando escrever um arquivo de teste em s3://{BUCKET_NAME}/{FOLDER_PATH}...\")\n",
    "    \n",
    "#     # Gera um nome de arquivo único para evitar conflitos\n",
    "#     test_key = f\"{FOLDER_PATH}/teste_acesso_{uuid.uuid4()}.txt\"\n",
    "#     test_content = \"Este é um arquivo de teste para verificar as permissões de escrita do SageMaker.\"\n",
    "\n",
    "#     s3_client.put_object(\n",
    "#         Bucket=BUCKET_NAME,\n",
    "#         Key=test_key,\n",
    "#         Body=test_content.encode('utf-8')\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\n✅ SUCESSO! O arquivo de teste '{test_key}' foi criado com sucesso.\")\n",
    "#     print(f\"Você pode verificar o arquivo no S3 em: https://s3.console.aws.amazon.com/s3/object/{BUCKET_NAME}?prefix={test_key}\")\n",
    "\n",
    "#     # 3. Tentar ler o arquivo de teste (verifica a permissão 's3:GetObject')\n",
    "#     print(\"\\nTentando ler o arquivo de teste...\")\n",
    "#     obj = s3_client.get_object(\n",
    "#         Bucket=BUCKET_NAME,\n",
    "#         Key=test_key\n",
    "#     )\n",
    "#     read_content = obj['Body'].read().decode('utf-8')\n",
    "#     print(f\"Conteúdo lido: '{read_content}'\")\n",
    "    \n",
    "#     # 4. Tentar remover o arquivo de teste (verifica a permissão 's3:DeleteObject')\n",
    "#     print(\"\\nTentando remover o arquivo de teste...\")\n",
    "#     s3_client.delete_object(\n",
    "#         Bucket=BUCKET_NAME,\n",
    "#         Key=test_key\n",
    "#     )\n",
    "#     print(\"✅ SUCESSO! O arquivo de teste foi removido.\")\n",
    "\n",
    "#     print(\"\\nO ambiente do SageMaker tem as permissões de leitura, escrita e exclusão corretas para o bucket.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"\\n❌ ERRO: Falha no teste de acesso. Detalhes do erro:\")\n",
    "#     print(e)\n",
    "#     if \"Access Denied\" in str(e):\n",
    "#         print(\"\\nO erro 'Access Denied' indica que o role IAM do seu notebook não tem permissão para realizar esta ação no S3.\")\n",
    "#     elif \"Invalid bucket name\" in str(e):\n",
    "#         print(\"\\nO erro 'Invalid bucket name' indica que o nome do bucket está incorreto. Verifique a variável BUCKET_NAME.\")\n",
    "#     else:\n",
    "#         print(\"\\nOcorreu um erro inesperado. Verifique a configuração do seu bucket e as políticas do seu role IAM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdeb44ca-a331-47f0-9435-c36132f33d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import boto3\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "CKAN_BASE = \"https://dados.ons.org.br/api/3/action\"\n",
    "SITE_BASE = CKAN_BASE.split(\"/api/3\", 1)[0]\n",
    "\n",
    "# --- Configurações de S3 CORRIGIDAS ---\n",
    "S3_BUCKET = \"labriscoenergeticomod2\"\n",
    "S3_FOLDER_RAW = \"raw\"\n",
    "S3_FOLDER_BRONZE = \"bronze\"\n",
    "S3_CLIENT = boto3.client(\"s3\")\n",
    "\n",
    "# Slugs preferidos no CKAN por dataset (evita selecionar pacotes de detalhamento)\n",
    "PREFERRED_SLUG_BY_QUERY: Dict[str, str] = {\n",
    "    \"Balanço de Energia nos Subsistemas\": \"balanco_energia_subsistema_ho\",\n",
    "    \"Intercâmbios Entre Subsistemas\": \"intercambio_nacional_ho\",\n",
    "    \"ENA Diário por Subsistema\": \"ena_subsistema_di\",\n",
    "    \"EAR Diário por Subsistema\": \"ear_subsistema_di\",\n",
    "    \"Restrição de Operação por Constrained-off de Usinas Eólicas\": \"restricao_coff_eolica_tm\",\n",
    "    \"Restrição de Operação por Constrained-off de Usinas Fotovoltaicas\": \"restricao_coff_fotovoltaica_tm\",\n",
    "    \"Carga Verificada\": \"carga_verificada_tm\",\n",
    "}\n",
    "\n",
    "# --- Funções auxiliares (sem alteração) ---\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Normaliza texto (minúsculas, sem acentos, sem pontuação extra).\"\"\"\n",
    "    import unicodedata\n",
    "    s = s.strip().lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _s3_upload_file_from_memory(buffer: io.BytesIO, bucket_name: str, s3_key: str):\n",
    "    \"\"\"Faz o upload de um buffer de bytes para o S3.\"\"\"\n",
    "    S3_CLIENT.put_object(Body=buffer.getvalue(), Bucket=bucket_name, Key=s3_key)\n",
    "\n",
    "def _is_s3_file_exists(bucket_name: str, s3_key: str) -> bool:\n",
    "    \"\"\"Verifica se um objeto existe no S3.\"\"\"\n",
    "    try:\n",
    "        S3_CLIENT.head_object(Bucket=bucket_name, Key=s3_key)\n",
    "        return True\n",
    "    except S3_CLIENT.exceptions.ClientError:\n",
    "        return False\n",
    "\n",
    "def _s3_download_file_to_memory(bucket_name: str, s3_key: str) -> io.BytesIO:\n",
    "    \"\"\"Baixa um objeto do S3 para um buffer de bytes na memória.\"\"\"\n",
    "    obj = S3_CLIENT.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "    return io.BytesIO(obj[\"Body\"].read())\n",
    "\n",
    "def _pick_resource(\n",
    "    resources: List[dict], prefer_formats: Tuple[str, ...] = (\"CSV\", \"XLSX\", \"ZIP\")\n",
    ") -> Optional[dict]:\n",
    "    \"\"\"Escolhe o melhor recurso (CSV/XLSX/ZIP mais recente) entre os disponíveis.\"\"\"\n",
    "    if not resources:\n",
    "        return None\n",
    "    def key(r):\n",
    "        fmt = (r.get(\"format\") or r.get(\"mimetype\") or \"\").upper()\n",
    "        try:\n",
    "            fmt_idx = prefer_formats.index(fmt)\n",
    "        except ValueError:\n",
    "            fmt_idx = len(prefer_formats)\n",
    "        last = r.get(\"last_modified\") or r.get(\"created\") or \"\"\n",
    "        return (fmt_idx, last)\n",
    "    return sorted(resources, key=key)[0]\n",
    "\n",
    "class CkanClient:\n",
    "    \"\"\"Cliente simplificado para interagir com a API CKAN do dados.ons.org.br.\"\"\"\n",
    "    def __init__(self, base: str = CKAN_BASE, timeout: int = 60):\n",
    "        self.base = base.rstrip(\"/\")\n",
    "        self.timeout = timeout\n",
    "    def _get(self, path: str, params: Optional[dict] = None) -> dict:\n",
    "        url = f\"{self.base}/{path.lstrip('/')}\"\n",
    "        r = requests.get(url, params=params, timeout=self.timeout)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "        if not j.get(\"success\"):\n",
    "            raise RuntimeError(f\"CKAN call failed: {url}\")\n",
    "        return j[\"result\"]\n",
    "    def search_package(self, query: str, rows: int = 10) -> List[dict]:\n",
    "        res = self._get(\"package_search\", {\"q\": query, \"rows\": rows})\n",
    "        return res.get(\"results\", [])\n",
    "    def show_package(self, package_id: str) -> dict:\n",
    "        return self._get(\"package_show\", {\"id\": package_id})\n",
    "\n",
    "@dataclass\n",
    "class DatasetSpec:\n",
    "    \"\"\"Especificação de dataset a baixar do CKAN.\"\"\"\n",
    "    query: str\n",
    "    out_name: str\n",
    "    note: str = \"\"\n",
    "    resource_filter: Optional[str] = None\n",
    "\n",
    "DEFAULT_SPECS: Dict[str, DatasetSpec] = {\n",
    "    \"balanco\": DatasetSpec(\n",
    "        query=\"Balanço de Energia nos Subsistemas\",\n",
    "        out_name=\"ons_balanco_subsistema_horario.csv\",\n",
    "        note=\"Usado para derivar geração por fonte (hidro/term/eolica/fv).\",\n",
    "        resource_filter=r\"(?i)hor(a|á)ria|hour|hora\",\n",
    "    ),\n",
    "    \"intercambio\": DatasetSpec(\n",
    "        query=\"Intercâmbios Entre Subsistemas\",\n",
    "        out_name=\"ons_intercambios_entre_subsistemas_horario.csv\",\n",
    "        note=\"Fluxos entre subsistemas para calcular import/export do SE/CO.\",\n",
    "        resource_filter=r\"(?i)hor(a|á)ria|hour|hora\",\n",
    "    ),\n",
    "    \"ena\": DatasetSpec(\n",
    "        query=\"ENA Diário por Subsistema\",\n",
    "        out_name=\"ons_ena_diario_subsistema.csv\",\n",
    "        note=\"ENA diária por subsistema (mwmed).\",\n",
    "        resource_filter=r\"(?i)di(á|a)rio|daily\",\n",
    "    ),\n",
    "    \"ear\": DatasetSpec(\n",
    "        query=\"EAR Diário por Subsistema\",\n",
    "        out_name=\"ons_ear_diario_subsistema.csv\",\n",
    "        note=\"EAR diária por subsistema (%).\",\n",
    "        resource_filter=r\"(?i)di(á|a)rio|daily\",\n",
    "    ),\n",
    "    \"corte_eolica\": DatasetSpec(\n",
    "        query=\"Restrição de Operação por Constrained-off de Usinas Eólicas\",\n",
    "        out_name=\"ons_constrained_off_eolica_mensal.csv\",\n",
    "        note=\"Cortes eólicos (mensal).\",\n",
    "        resource_filter=r\"(?i)mensal|monthly\",\n",
    "    ),\n",
    "    \"corte_fv\": DatasetSpec(\n",
    "        query=\"Restrição de Operação por Constrained-off de Usinas Fotovoltaicas\",\n",
    "        out_name=\"ons_constrained_off_fv_mensal.csv\",\n",
    "        note=\"Cortes FV (mensal).\",\n",
    "        resource_filter=r\"(?i)mensal|monthly\",\n",
    "    ),\n",
    "    \"carga\": DatasetSpec(\n",
    "        query=\"Carga Verificada\",\n",
    "        out_name=\"ons_carga.csv\",\n",
    "        note=\"Carga verificada (horária ou diária).\",\n",
    "        resource_filter=None,\n",
    "    ),\n",
    "}\n",
    "\n",
    "def _parse_since_to_date(since: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Converte um ano ou ano-mês em uma string de data completa.\"\"\"\n",
    "    if not since:\n",
    "        return None\n",
    "    try:\n",
    "        parts = since.split(\"-\")\n",
    "        if len(parts) == 1 and len(parts[0]) == 4:\n",
    "            return f\"{int(parts[0]):04d}-01-01\"\n",
    "        elif len(parts) >= 2:\n",
    "            return f\"{int(parts[0]):04d}-{int(parts[1]):02d}-01\"\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def fetch_carga_api(\n",
    "    out_s3_key: str,\n",
    "    since: Optional[str] = None,\n",
    "    until: Optional[str] = None,\n",
    "    area: str = \"SECO\",\n",
    "    overwrite: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Baixa Carga Verificada via API e salva em S3.\"\"\"\n",
    "    # CORRIGIDO: Define a chave S3 completa combinando a pasta e o nome do arquivo\n",
    "    s3_key_full = f\"{S3_FOLDER_RAW}/{out_s3_key}\"\n",
    "\n",
    "    if _is_s3_file_exists(S3_BUCKET, s3_key_full) and not overwrite:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[CKAN] Já existe s3://{S3_BUCKET}/{s3_key_full}; pulando (use --overwrite para baixar novamente)\"\n",
    "            )\n",
    "        return s3_key_full\n",
    "    \n",
    "    start_str = _parse_since_to_date(since) or (datetime.utcnow().strftime(\"%Y-%m-01\"))\n",
    "    start = pd.to_datetime(start_str)\n",
    "    end = (\n",
    "        pd.to_datetime(until)\n",
    "        if until\n",
    "        else pd.to_datetime(datetime.utcnow().strftime(\"%Y-%m-%d\"))\n",
    "    )\n",
    "    if end < start:\n",
    "        end = start\n",
    "\n",
    "    base_url = \"https://apicarga.ons.org.br/prd/cargaverificada\"\n",
    "    step = pd.Timedelta(days=89)\n",
    "\n",
    "    dfs = []\n",
    "    cur = start\n",
    "    while cur <= end:\n",
    "        j_ini = cur.strftime(\"%Y-%m-%d\")\n",
    "        j_fim = min(cur + step, end).strftime(\"%Y-%m-%d\")\n",
    "        params = {\"dat_inicio\": j_ini, \"dat_fim\": j_fim, \"cod_areacarga\": area}\n",
    "        if verbose:\n",
    "            print(f\"[ONS API] Carga {j_ini} -> {j_fim} ({area})\")\n",
    "        try:\n",
    "            r = requests.get(base_url, params=params, timeout=(15, 60))\n",
    "            r.raise_for_status()\n",
    "            j = r.json()\n",
    "            if isinstance(j, dict):\n",
    "                arr = next((v for v in j.values() if isinstance(v, list)), [])\n",
    "            elif isinstance(j, list):\n",
    "                arr = j\n",
    "            else:\n",
    "                arr = []\n",
    "            if not arr:\n",
    "                cur += step + pd.Timedelta(days=1)\n",
    "                continue\n",
    "            df = pd.DataFrame(arr)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"[ONS API] Falha {j_ini}->{j_fim}: {e}\")\n",
    "        cur += step + pd.Timedelta(days=1)\n",
    "\n",
    "    if not dfs:\n",
    "        if verbose:\n",
    "            print(\"[ONS API] Nenhum dado retornado para Carga.\")\n",
    "        return None\n",
    "\n",
    "    raw = pd.concat(dfs, ignore_index=True)\n",
    "    buffer = io.BytesIO()\n",
    "    raw.to_csv(buffer, index=False)\n",
    "    buffer.seek(0)\n",
    "    # CORRIGIDO: Usa a variável S3_BUCKET e a chave completa\n",
    "    _s3_upload_file_from_memory(buffer, S3_BUCKET, s3_key_full)\n",
    "    return s3_key_full\n",
    "\n",
    "def _download_resource_to_memory(url: str) -> io.BytesIO:\n",
    "    \"\"\"Baixa um recurso de uma URL para um buffer de bytes na memória.\"\"\"\n",
    "    with requests.get(url, stream=True, timeout=(15, 300)) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"Content-Length\", 0))\n",
    "        read = 0\n",
    "        last_mb_print = 0\n",
    "        buffer = io.BytesIO()\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024):  # 1MB\n",
    "            if not chunk:\n",
    "                continue\n",
    "            buffer.write(chunk)\n",
    "            read += len(chunk)\n",
    "            if total and (read - last_mb_print) >= 50 * 1024 * 1024:\n",
    "                mb = read / (1024 * 1024)\n",
    "                tot_mb = total / (1024 * 1024)\n",
    "                print(f\"  [download] {mb:.0f}/{tot_mb:.0f} MB\")\n",
    "                last_mb_print = read\n",
    "        buffer.seek(0)\n",
    "        return buffer\n",
    "\n",
    "def _maybe_convert_to_csv_in_memory(in_buffer: io.BytesIO, file_url: str) -> io.BytesIO:\n",
    "    \"\"\"Converte um arquivo (XLS, XLSX, ZIP) para um buffer CSV em memória.\"\"\"\n",
    "    ext = Path(file_url).suffix.lower()\n",
    "    df = None\n",
    "    if ext == \".csv\":\n",
    "        in_buffer.seek(0)\n",
    "        try:\n",
    "            df = pd.read_csv(in_buffer, sep=None, engine=\"python\")\n",
    "        except Exception:\n",
    "            in_buffer.seek(0)\n",
    "            df = pd.read_csv(in_buffer, encoding='latin1', sep=None, engine=\"python\")\n",
    "    elif ext in (\".xlsx\", \".xls\"):\n",
    "        df = pd.read_excel(in_buffer)\n",
    "    elif ext == \".zip\":\n",
    "        with zipfile.ZipFile(in_buffer, \"r\") as z:\n",
    "            infos = sorted(z.infolist(), key=lambda i: i.file_size, reverse=True)\n",
    "            choice = None\n",
    "            for info in infos:\n",
    "                name = info.filename.lower()\n",
    "                if name.endswith(\".csv\") or name.endswith(\".xlsx\"):\n",
    "                    choice = info\n",
    "                    break\n",
    "            if choice is None:\n",
    "                choice = infos[0]\n",
    "            with z.open(choice) as f:\n",
    "                content = f.read()\n",
    "            if choice.filename.lower().endswith(\".csv\"):\n",
    "                df = pd.read_csv(io.BytesIO(content), sep=None, engine=\"python\")\n",
    "            else:\n",
    "                df = pd.read_excel(io.BytesIO(content))\n",
    "    else:\n",
    "        try:\n",
    "            in_buffer.seek(0)\n",
    "            df = pd.read_csv(in_buffer, sep=None, engine=\"python\")\n",
    "        except Exception:\n",
    "            in_buffer.seek(0)\n",
    "            try:\n",
    "                df = pd.read_excel(in_buffer)\n",
    "            except Exception:\n",
    "                raise ValueError(\"Formato de arquivo não suportado\")\n",
    "    if df is not None:\n",
    "        out_buffer = io.BytesIO()\n",
    "        df.to_csv(out_buffer, index=False)\n",
    "        out_buffer.seek(0)\n",
    "        return out_buffer\n",
    "    raise ValueError(\"Não foi possível converter o arquivo para CSV.\")\n",
    "\n",
    "def _append_csv_buffers(sources: List[io.BytesIO], target: io.BytesIO):\n",
    "    \"\"\"Concatena múltiplos buffers CSV em um único buffer `target`.\"\"\"\n",
    "    if not sources:\n",
    "        raise ValueError(\"Nenhuma fonte para concatenar\")\n",
    "    wrote_any = False\n",
    "    for i, src in enumerate(sources):\n",
    "        src.seek(0)\n",
    "        if not wrote_any:\n",
    "            target.write(src.read())\n",
    "            wrote_any = True\n",
    "        else:\n",
    "            src.readline()\n",
    "            target.write(src.read())\n",
    "\n",
    "def _parse_iso_dt(s: Optional[str]) -> Optional[datetime]:\n",
    "    \"\"\"Converte uma string de data (formato ISO) em um objeto datetime.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    for fmt in (\"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(s.split(\"Z\")[0], fmt)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _extract_period_ym_from_resource(r: dict) -> Optional[Tuple[int, Optional[int]]]:\n",
    "    \"\"\"Extrai (ano, mes?) do recurso a partir do nome/descrição/URL ou metadados.\"\"\"\n",
    "    key = _period_key_from_resource(r)\n",
    "    m = re.match(r\"^(\\d{4})(?:-(\\d{2}))?$\", key)\n",
    "    if m:\n",
    "        y = int(m.group(1))\n",
    "        mth = int(m.group(2)) if m.group(2) else None\n",
    "        return y, mth\n",
    "    return None\n",
    "\n",
    "def _parse_since_until(\n",
    "    since: Optional[str], until: Optional[str]\n",
    ") -> Tuple[Optional[Tuple[int, Optional[int]]], Optional[Tuple[int, Optional[int]]]]:\n",
    "    \"\"\"Analisa as strings 'since' e 'until' em tuplas (ano, mês).\"\"\"\n",
    "    def _p(val: Optional[str]) -> Optional[Tuple[int, Optional[int]]]:\n",
    "        if not val:\n",
    "            return None\n",
    "        m = re.match(r\"^(\\d{4})(?:-(\\d{2}))?$\", val)\n",
    "        if not m:\n",
    "            return None\n",
    "        y = int(m.group(1))\n",
    "        mth = int(m.group(2)) if m.group(2) else None\n",
    "        return y, mth\n",
    "    return _p(since), _p(until)\n",
    "\n",
    "def _resource_matches_since_until(\n",
    "    r: dict, since: Optional[str], until: Optional[str]\n",
    ") -> bool:\n",
    "    \"\"\"Decide se um recurso entra no intervalo.\"\"\"\n",
    "    if not since and not until:\n",
    "        return True\n",
    "    r_ym = _extract_period_ym_from_resource(r)\n",
    "    s_ym, u_ym = _parse_since_until(since, until)\n",
    "    if r_ym:\n",
    "        ry, rm = r_ym\n",
    "        ok_since = True\n",
    "        ok_until = True\n",
    "        if s_ym:\n",
    "            sy, sm = s_ym\n",
    "            ok_since = (ry > sy) or (\n",
    "                ry == sy and (sm is None or (rm is not None and rm >= sm))\n",
    "            )\n",
    "        if u_ym:\n",
    "            uy, um = u_ym\n",
    "            ok_until = (ry < uy) or (\n",
    "                ry == uy and (um is None or (rm is not None and rm <= um))\n",
    "            )\n",
    "        return ok_since and ok_until\n",
    "    name = (r.get(\"name\") or \"\") + \" \" + (r.get(\"description\") or \"\")\n",
    "    ok_since = True if not since else (since in name)\n",
    "    ok_until = True if not until else (until in name)\n",
    "    return ok_since and ok_until\n",
    "\n",
    "def _list_resources(\n",
    "    client: CkanClient, query: str, resource_filter: Optional[str], verbose: bool\n",
    ") -> Tuple[dict, List[dict]]:\n",
    "    \"\"\"Busca o pacote mais relevante para uma query e lista seus recursos.\"\"\"\n",
    "    pkgs = client.search_package(query, rows=50)\n",
    "    if not pkgs:\n",
    "        if verbose:\n",
    "            print(f\"[CKAN] Nenhum pacote encontrado para query: {query}\")\n",
    "        return {}, []\n",
    "    qn = _norm(query)\n",
    "    pref_slug = PREFERRED_SLUG_BY_QUERY.get(query, \"\").lower()\n",
    "    def score(pkg):\n",
    "        title = _norm(pkg.get(\"title\", \"\"))\n",
    "        s = 0\n",
    "        if qn in title or title in qn:\n",
    "            s += 10\n",
    "        mm = pkg.get(\"metadata_modified\") or \"\"\n",
    "        name = (pkg.get(\"name\") or \"\").lower()\n",
    "        if pref_slug and pref_slug in name:\n",
    "            s += 500\n",
    "        if (\"detalh\" in title) or (\"detail\" in title) or (\"detail\" in name):\n",
    "            s -= 200\n",
    "        return (s, mm)\n",
    "    pkg = None\n",
    "    if pref_slug:\n",
    "        for p in pkgs:\n",
    "            if pref_slug in (p.get(\"name\") or \"\").lower():\n",
    "                pkg = p\n",
    "                break\n",
    "    if pkg is None:\n",
    "        pkg = sorted(pkgs, key=score, reverse=True)[0]\n",
    "    pkg_full = client.show_package(pkg[\"id\"]) if \"id\" in pkg else pkg\n",
    "    resources = pkg_full.get(\"resources\", [])\n",
    "    if resource_filter:\n",
    "        rx = re.compile(resource_filter)\n",
    "        filtered = [\n",
    "            r\n",
    "            for r in resources\n",
    "            if rx.search((r.get(\"name\") or \"\") + \" \" + (r.get(\"description\") or \"\"))\n",
    "        ]\n",
    "        resources = filtered or resources\n",
    "    return pkg_full, resources\n",
    "\n",
    "def _pref_index(\n",
    "    fmt: Optional[str], prefer: Tuple[str, ...] = (\"CSV\", \"XLSX\", \"ZIP\")\n",
    ") -> int:\n",
    "    \"\"\"Retorna o índice de preferência de um formato de arquivo.\"\"\"\n",
    "    f = (fmt or \"\").upper()\n",
    "    try:\n",
    "        return prefer.index(f)\n",
    "    except ValueError:\n",
    "        return len(prefer)\n",
    "\n",
    "_RE_YM = re.compile(r\"(20\\d{2})[-_/.]?(0[1-9]|1[0-2])\")\n",
    "_RE_Y = re.compile(r\"(19\\d{2}|20\\d{2})\")\n",
    "\n",
    "def _period_key_from_resource(r: dict) -> str:\n",
    "    \"\"\"Extrai uma chave de período (YYYY-MM ou YYYY) do nome/descrição/URL do recurso.\"\"\"\n",
    "    name = (\n",
    "        (r.get(\"name\") or \"\")\n",
    "        + \" \"\n",
    "        + (r.get(\"description\") or \"\")\n",
    "        + \" \"\n",
    "        + (r.get(\"url\") or \"\")\n",
    "    )\n",
    "    m = _RE_YM.search(name)\n",
    "    if m:\n",
    "        return f\"{m.group(1)}-{m.group(2)}\"\n",
    "    m = _RE_Y.search(name)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    lm = _parse_iso_dt(r.get(\"last_modified\") or r.get(\"created\"))\n",
    "    if lm:\n",
    "        return f\"{lm.year:04d}-{lm.month:02d}\"\n",
    "    return (r.get(\"name\") or \"\").strip() or (r.get(\"id\") or \"unknown\")\n",
    "\n",
    "def _group_prefer_by_period(\n",
    "    resources: List[dict], prefer: Tuple[str, ...] = (\"CSV\", \"XLSX\", \"ZIP\")\n",
    ") -> List[dict]:\n",
    "    \"\"\"Agrupa recursos por período e escolhe o melhor formato para cada período.\"\"\"\n",
    "    groups: Dict[str, List[dict]] = {}\n",
    "    for r in resources:\n",
    "        k = _period_key_from_resource(r)\n",
    "        groups.setdefault(k, []).append(r)\n",
    "    chosen: List[dict] = []\n",
    "    for k, items in groups.items():\n",
    "        items_sorted = sorted(\n",
    "            items,\n",
    "            key=lambda r: (\n",
    "                _pref_index(r.get(\"format\"), prefer),\n",
    "                r.get(\"last_modified\") or r.get(\"created\") or \"\",\n",
    "            ),\n",
    "        )\n",
    "        chosen.append(items_sorted[0])\n",
    "    chosen = sorted(chosen, key=lambda r: _period_key_from_resource(r))\n",
    "    return chosen\n",
    "\n",
    "def _is_data_resource(r: dict) -> bool:\n",
    "    \"\"\"Retorna True se o recurso parece ser dado e não documentação.\"\"\"\n",
    "    fmt = (r.get(\"format\") or r.get(\"mimetype\") or \"\").upper()\n",
    "    url = (r.get(\"url\") or r.get(\"download_url\") or \"\").lower()\n",
    "    name = ((r.get(\"name\") or \"\") + \" \" + (r.get(\"description\") or \"\")).lower()\n",
    "    if any(\n",
    "        bad in name\n",
    "        for bad in [\"dicionario\", \"dictionary\", \"glossario\", \"metadado\", \"metadata\"]\n",
    "    ):\n",
    "        return False\n",
    "    if fmt in {\"CSV\", \"XLSX\", \"XLS\", \"ZIP\"}:\n",
    "        return True\n",
    "    if (\n",
    "        url.endswith(\".csv\")\n",
    "        or url.endswith(\".xlsx\")\n",
    "        or url.endswith(\".xls\")\n",
    "        or url.endswith(\".zip\")\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def fetch_one(\n",
    "    client: CkanClient,\n",
    "    spec: DatasetSpec,\n",
    "    out_s3_key: str,\n",
    "    verbose: bool = True,\n",
    "    since: Optional[str] = None,\n",
    "    until: Optional[str] = None,\n",
    "    overwrite: bool = False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Baixa um dataset do ONS e salva no S3.\"\"\"\n",
    "    # CORRIGIDO: Define a chave S3 completa\n",
    "    s3_key_full = f\"{S3_FOLDER_RAW}/{out_s3_key}\"\n",
    "    \n",
    "    # CORRIGIDO: Verifica a existência com o nome do bucket correto e a chave completa\n",
    "    if _is_s3_file_exists(S3_BUCKET, s3_key_full) and not overwrite:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[CKAN] Já existe s3://{S3_BUCKET}/{s3_key_full}; pulando (use --overwrite para baixar novamente)\"\n",
    "            )\n",
    "        return s3_key_full\n",
    "    pkg_full, resources = _list_resources(\n",
    "        client, spec.query, spec.resource_filter, verbose\n",
    "    )\n",
    "    if not resources:\n",
    "        return None\n",
    "    resources = [r for r in resources if _is_data_resource(r)]\n",
    "    if since or until:\n",
    "        candidates = [\n",
    "            r for r in resources if _resource_matches_since_until(r, since, until)\n",
    "        ] or resources\n",
    "        res = _pick_resource(candidates)\n",
    "    else:\n",
    "        res = _pick_resource(resources)\n",
    "    if not res:\n",
    "        if verbose:\n",
    "            print(f\"[CKAN] Nenhum recurso adequado para: {spec.query}\")\n",
    "        return None\n",
    "    url = res.get(\"url\") or res.get(\"download_url\")\n",
    "    if not url:\n",
    "        if verbose:\n",
    "            print(f\"[CKAN] Recurso sem URL para: {spec.query}\")\n",
    "        return None\n",
    "    if verbose:\n",
    "        page_slug = pkg_full.get(\"name\") or pkg_full.get(\"id\") or \"\"\n",
    "        page_url = f\"{SITE_BASE}/dataset/{page_slug}\" if page_slug else SITE_BASE\n",
    "        res_name = (res.get(\"name\") or \"\").strip()\n",
    "        res_fmt = (res.get(\"format\") or res.get(\"mimetype\") or \"\").upper()\n",
    "        res_last = res.get(\"last_modified\") or res.get(\"created\") or \"\"\n",
    "        print(f\"[CKAN] Pacote: {pkg_full.get('title','(sem título)')} -> {page_url}\")\n",
    "        print(f\"[CKAN] Recurso: {res_name} [{res_fmt}] {res_last}\")\n",
    "        print(f\"[CKAN] URL: {url}\")\n",
    "    if verbose:\n",
    "        print(f\"[CKAN] Baixando {spec.query} -> {s3_key_full}\")\n",
    "    try:\n",
    "        downloaded_buffer = _download_resource_to_memory(url)\n",
    "        csv_buffer = _maybe_convert_to_csv_in_memory(downloaded_buffer, url)\n",
    "        # CORRIGIDO: Usa a variável S3_BUCKET e a chave completa\n",
    "        _s3_upload_file_from_memory(csv_buffer, S3_BUCKET, s3_key_full)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"[CKAN] Falha no processamento: {e}\")\n",
    "        return None\n",
    "    return s3_key_full\n",
    "\n",
    "def fetch_many_and_concat(\n",
    "    client: CkanClient,\n",
    "    spec: DatasetSpec,\n",
    "    out_s3_key: str,\n",
    "    since: Optional[str] = None,\n",
    "    until: Optional[str] = None,\n",
    "    verbose: bool = True,\n",
    "    overwrite: bool = False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Baixa todos os recursos do pacote, concatena e salva no S3.\"\"\"\n",
    "    # CORRIGIDO: Define a chave S3 completa\n",
    "    s3_key_full = f\"{S3_FOLDER_RAW}/{out_s3_key}\"\n",
    "\n",
    "    # CORRIGIDO: Verifica a existência com o nome do bucket correto e a chave completa\n",
    "    if _is_s3_file_exists(S3_BUCKET, s3_key_full) and not overwrite:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[CKAN] Já existe s3://{S3_BUCKET}/{s3_key_full}; pulando concatenação (use --overwrite)\"\n",
    "            )\n",
    "        return s3_key_full\n",
    "    pkg_full, resources = _list_resources(\n",
    "        client, spec.query, spec.resource_filter, verbose\n",
    "    )\n",
    "    if not resources:\n",
    "        return None\n",
    "    def rkey(r):\n",
    "        return r.get(\"last_modified\") or r.get(\"created\") or r.get(\"name\") or \"\"\n",
    "    resources = sorted(resources, key=rkey)\n",
    "    resources = [r for r in resources if _is_data_resource(r)]\n",
    "    resources = [r for r in resources if _resource_matches_since_until(r, since, until)]\n",
    "    resources = _group_prefer_by_period(resources)\n",
    "    csv_buffers = []\n",
    "    for r in resources:\n",
    "        url = r.get(\"url\") or r.get(\"download_url\")\n",
    "        if not url:\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f\"[CKAN] Recurso mensal: {r.get('name','(sem nome)')} -> {url}\")\n",
    "        try:\n",
    "            downloaded_buffer = _download_resource_to_memory(url)\n",
    "            csv_buffer = _maybe_convert_to_csv_in_memory(downloaded_buffer, url)\n",
    "            csv_buffers.append(csv_buffer)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"[CKAN] Falha no processamento do recurso: {e}\")\n",
    "            continue\n",
    "    if not csv_buffers:\n",
    "        if verbose:\n",
    "            print(f\"[CKAN] Nenhum recurso para concatenar para: {spec.query}\")\n",
    "        return None\n",
    "    final_buffer = io.BytesIO()\n",
    "    _append_csv_buffers(csv_buffers, final_buffer)\n",
    "    final_buffer.seek(0)\n",
    "    # CORRIGIDO: Usa a variável S3_BUCKET e a chave completa\n",
    "    _s3_upload_file_from_memory(final_buffer, S3_BUCKET, s3_key_full)\n",
    "    return s3_key_full\n",
    "\n",
    "def fetch_all(\n",
    "    datasets: Optional[List[str]] = None,\n",
    "    verbose: bool = True,\n",
    "    since: Optional[str] = None,\n",
    "    until: Optional[str] = None,\n",
    "    overwrite: bool = False,\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Baixa todos os datasets especificados (ou os padrão) e salva no S3.\"\"\"\n",
    "    if datasets is None:\n",
    "        datasets = list(DEFAULT_SPECS.keys())\n",
    "    client = CkanClient()\n",
    "    results: Dict[str, Optional[str]] = {}\n",
    "    for key in datasets:\n",
    "        spec = DEFAULT_SPECS[key]\n",
    "        out_s3_key = spec.out_name\n",
    "        try:\n",
    "            if key in {\n",
    "                \"corte_eolica\",\n",
    "                \"corte_fv\",\n",
    "                \"balanco\",\n",
    "                \"intercambio\",\n",
    "                \"ena\",\n",
    "                \"ear\",\n",
    "            }:\n",
    "                p = fetch_many_and_concat(\n",
    "                    client,\n",
    "                    spec,\n",
    "                    out_s3_key,\n",
    "                    since=since,\n",
    "                    until=until,\n",
    "                    verbose=verbose,\n",
    "                    overwrite=overwrite,\n",
    "                )\n",
    "            elif key == \"carga\":\n",
    "                p = fetch_carga_api(\n",
    "                    out_s3_key,\n",
    "                    since=since,\n",
    "                    until=until,\n",
    "                    overwrite=overwrite,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "            else:\n",
    "                p = fetch_one(\n",
    "                    client,\n",
    "                    spec,\n",
    "                    out_s3_key,\n",
    "                    verbose=verbose,\n",
    "                    since=since,\n",
    "                    until=until,\n",
    "                    overwrite=overwrite,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] {key}: {e}\")\n",
    "            p = None\n",
    "        results[key] = p\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436fc0f3-b35f-455c-82b2-7c46db9261a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processo de download e upload para o S3...\n",
      "[CKAN] Recurso mensal: Balanco_de_Energia_Subsistema-2019 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/balanco_energia_subsistema_ho/BALANCO_ENERGIA_SUBSISTEMA_2019.csv\n",
      "[CKAN] Recurso mensal: Balanco_de_Energia_Subsistema-2020 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/balanco_energia_subsistema_ho/BALANCO_ENERGIA_SUBSISTEMA_2020.csv\n",
      "[CKAN] Recurso mensal: Balanco_de_Energia_Subsistema-2021 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/balanco_energia_subsistema_ho/BALANCO_ENERGIA_SUBSISTEMA_2021.csv\n",
      "[CKAN] Recurso mensal: Balanco_de_Energia_Subsistema-2022 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/balanco_energia_subsistema_ho/BALANCO_ENERGIA_SUBSISTEMA_2022.csv\n",
      "[CKAN] Recurso mensal: Balanco_de_Energia_Subsistema-2023 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/balanco_energia_subsistema_ho/BALANCO_ENERGIA_SUBSISTEMA_2023.csv\n",
      "[CKAN] Recurso mensal: Balanco_de_Energia_Subsistema-2024 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/balanco_energia_subsistema_ho/BALANCO_ENERGIA_SUBSISTEMA_2024.csv\n",
      "[CKAN] Recurso mensal: Balanco_de_Energia_Subsistema-2025 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/balanco_energia_subsistema_ho/BALANCO_ENERGIA_SUBSISTEMA_2025.csv\n",
      "[CKAN] Recurso mensal: Intercambio_Nacional-2019 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/intercambio_nacional_ho/INTERCAMBIO_NACIONAL_2019.csv\n",
      "[CKAN] Recurso mensal: Intercambio_Nacional-2020 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/intercambio_nacional_ho/INTERCAMBIO_NACIONAL_2020.csv\n",
      "[CKAN] Recurso mensal: Intercambio_Nacional-2021 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/intercambio_nacional_ho/INTERCAMBIO_NACIONAL_2021.csv\n",
      "[CKAN] Recurso mensal: Intercambio_Nacional-2022 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/intercambio_nacional_ho/INTERCAMBIO_NACIONAL_2022.csv\n",
      "[CKAN] Recurso mensal: Intercambio_Nacional-2023 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/intercambio_nacional_ho/INTERCAMBIO_NACIONAL_2023.csv\n",
      "[CKAN] Recurso mensal: Intercambio_Nacional-2024 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/intercambio_nacional_ho/INTERCAMBIO_NACIONAL_2024.csv\n",
      "[CKAN] Recurso mensal: Intercambio_Nacional-2025 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/intercambio_nacional_ho/INTERCAMBIO_NACIONAL_2025.csv\n",
      "[CKAN] Recurso mensal: ENA_Diario_por_Subsistema-2019 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ena_subsistema_di/ENA_DIARIO_SUBSISTEMA_2019.csv\n",
      "[CKAN] Recurso mensal: ENA_Diario_por_Subsistema-2020 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ena_subsistema_di/ENA_DIARIO_SUBSISTEMA_2020.csv\n",
      "[CKAN] Recurso mensal: ENA_Diario_por_Subsistema-2021 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ena_subsistema_di/ENA_DIARIO_SUBSISTEMA_2021.csv\n",
      "[CKAN] Recurso mensal: ENA_Diario_por_Subsistema-2022 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ena_subsistema_di/ENA_DIARIO_SUBSISTEMA_2022.csv\n",
      "[CKAN] Recurso mensal: ENA_Diario_por_Subsistema-2023 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ena_subsistema_di/ENA_DIARIO_SUBSISTEMA_2023.csv\n",
      "[CKAN] Recurso mensal: ENA_Diario_por_Subsistema-2024 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ena_subsistema_di/ENA_DIARIO_SUBSISTEMA_2024.csv\n",
      "[CKAN] Recurso mensal: ENA_Diario_por_Subsistema-2025 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ena_subsistema_di/ENA_DIARIO_SUBSISTEMA_2025.csv\n",
      "[CKAN] Recurso mensal: EAR_Diario_por_Subsistema-2019 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ear_subsistema_di/EAR_DIARIO_SUBSISTEMA_2019.csv\n",
      "[CKAN] Recurso mensal: EAR_Diario_por_Subsistema-2020 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ear_subsistema_di/EAR_DIARIO_SUBSISTEMA_2020.csv\n",
      "[CKAN] Recurso mensal: EAR_Diario_por_Subsistema-2021 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ear_subsistema_di/EAR_DIARIO_SUBSISTEMA_2021.csv\n",
      "[CKAN] Recurso mensal: EAR_Diario_por_Subsistema-2022 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ear_subsistema_di/EAR_DIARIO_SUBSISTEMA_2022.csv\n",
      "[CKAN] Recurso mensal: EAR_Diario_por_Subsistema-2023 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ear_subsistema_di/EAR_DIARIO_SUBSISTEMA_2023.csv\n",
      "[CKAN] Recurso mensal: EAR_Diario_por_Subsistema-2024 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ear_subsistema_di/EAR_DIARIO_SUBSISTEMA_2024.csv\n",
      "[CKAN] Recurso mensal: EAR_Diario_por_Subsistema-2025 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/ear_subsistema_di/EAR_DIARIO_SUBSISTEMA_2025.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2021-10 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2021_10.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2021-11 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2021_11.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2021-12 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2021_12.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-01 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_01.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-02 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_02.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-03 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_03.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-04 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_04.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-05 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_05.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-06 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_06.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-07 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_07.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-08 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_08.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-09 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_09.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-10 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_10.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-11 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_11.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2022-12 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2022_12.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-01 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_01.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-02 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_02.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-03 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_03.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-04 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_04.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-05 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_05.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-06 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_06.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-07 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_07.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-08 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_08.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-09 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_09.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-10 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_10.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-11 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_11.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2023-12 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2023_12.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-01 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_01.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-02 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_02.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-03 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_03.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-04 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_04.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-05 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_05.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-06 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_06.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-07 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_07.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-08 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_08.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-09 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_09.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-10 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_10.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-11 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_11.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2024-12 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2024_12.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-01 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_01.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-02 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_02.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-03 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_03.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-04 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_04.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-05 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_05.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-06 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_06.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-07 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_07.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-08 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_08.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-09 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_09.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_eolicas-2025-10 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_eolica_tm/RESTRICAO_COFF_EOLICA_2025_10.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-04 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_04.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-05 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_05.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-06 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_06.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-07 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_07.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-08 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_08.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-09 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_09.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-10 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_10.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-11 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_11.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2024-12 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2024_12.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-01 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_01.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-02 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_02.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-03 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_03.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-04 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_04.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-05 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_05.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-06 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_06.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-07 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_07.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-08 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_08.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-09 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_09.csv\n",
      "[CKAN] Recurso mensal: Restricoes_coff_fotovoltaica-2025-10 -> https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/restricao_coff_fotovoltaica_tm/RESTRICAO_COFF_FOTOVOLTAICA_2025_10.csv\n",
      "[ONS API] Carga 2018-01-01 -> 2018-03-31 (SECO)\n",
      "[ONS API] Falha 2018-01-01->2018-03-31: Expecting value: line 9 column 33 (char 318)\n",
      "[ONS API] Carga 2018-04-01 -> 2018-06-29 (SECO)\n",
      "[ONS API] Falha 2018-04-01->2018-06-29: Expecting value: line 9 column 33 (char 318)\n",
      "[ONS API] Carga 2018-06-30 -> 2018-09-27 (SECO)\n",
      "[ONS API] Falha 2018-06-30->2018-09-27: Expecting value: line 9 column 33 (char 318)\n",
      "[ONS API] Carga 2018-09-28 -> 2018-12-26 (SECO)\n",
      "[ONS API] Falha 2018-09-28->2018-12-26: Expecting value: line 9 column 33 (char 318)\n",
      "[ONS API] Carga 2018-12-27 -> 2019-03-26 (SECO)\n",
      "[ONS API] Falha 2018-12-27->2019-03-26: Expecting value: line 9 column 33 (char 318)\n",
      "[ONS API] Carga 2019-03-27 -> 2019-06-24 (SECO)\n",
      "[ONS API] Carga 2019-06-25 -> 2019-09-22 (SECO)\n",
      "[ONS API] Carga 2019-09-23 -> 2019-12-21 (SECO)\n",
      "[ONS API] Carga 2019-12-22 -> 2020-03-20 (SECO)\n",
      "[ONS API] Carga 2020-03-21 -> 2020-06-18 (SECO)\n",
      "[ONS API] Carga 2020-06-19 -> 2020-09-16 (SECO)\n",
      "[ONS API] Carga 2020-09-17 -> 2020-12-15 (SECO)\n",
      "[ONS API] Carga 2020-12-16 -> 2021-03-15 (SECO)\n",
      "[ONS API] Carga 2021-03-16 -> 2021-06-13 (SECO)\n",
      "[ONS API] Carga 2021-06-14 -> 2021-09-11 (SECO)\n",
      "[ONS API] Carga 2021-09-12 -> 2021-12-10 (SECO)\n",
      "[ONS API] Carga 2021-12-11 -> 2022-03-10 (SECO)\n",
      "[ONS API] Carga 2022-03-11 -> 2022-06-08 (SECO)\n",
      "[ONS API] Carga 2022-06-09 -> 2022-09-06 (SECO)\n",
      "[ONS API] Carga 2022-09-07 -> 2022-12-05 (SECO)\n",
      "[ONS API] Carga 2022-12-06 -> 2023-03-05 (SECO)\n",
      "[ONS API] Carga 2023-03-06 -> 2023-06-03 (SECO)\n",
      "[ONS API] Carga 2023-06-04 -> 2023-09-01 (SECO)\n",
      "[ONS API] Carga 2023-09-02 -> 2023-11-30 (SECO)\n",
      "[ONS API] Carga 2023-12-01 -> 2024-02-28 (SECO)\n",
      "[ONS API] Carga 2024-02-29 -> 2024-05-28 (SECO)\n",
      "[ONS API] Carga 2024-05-29 -> 2024-08-26 (SECO)\n",
      "[ONS API] Carga 2024-08-27 -> 2024-11-24 (SECO)\n",
      "[ONS API] Carga 2024-11-25 -> 2025-02-22 (SECO)\n",
      "[ONS API] Carga 2025-02-23 -> 2025-05-23 (SECO)\n",
      "[ONS API] Carga 2025-05-24 -> 2025-08-21 (SECO)\n",
      "[ONS API] Carga 2025-08-22 -> 2025-10-07 (SECO)\n",
      "\n",
      "--- Relatório Final de Processamento ---\n",
      "✅ SUCESSO! Dataset 'balanco' salvo em: s3://labriscoenergeticomod2/raw/ons_balanco_subsistema_horario.csv\n",
      "✅ SUCESSO! Dataset 'intercambio' salvo em: s3://labriscoenergeticomod2/raw/ons_intercambios_entre_subsistemas_horario.csv\n",
      "✅ SUCESSO! Dataset 'ena' salvo em: s3://labriscoenergeticomod2/raw/ons_ena_diario_subsistema.csv\n",
      "✅ SUCESSO! Dataset 'ear' salvo em: s3://labriscoenergeticomod2/raw/ons_ear_diario_subsistema.csv\n",
      "✅ SUCESSO! Dataset 'corte_eolica' salvo em: s3://labriscoenergeticomod2/raw/ons_constrained_off_eolica_mensal.csv\n",
      "✅ SUCESSO! Dataset 'corte_fv' salvo em: s3://labriscoenergeticomod2/raw/ons_constrained_off_fv_mensal.csv\n",
      "✅ SUCESSO! Dataset 'carga' salvo em: s3://labriscoenergeticomod2/raw/ons_carga.csv\n"
     ]
    }
   ],
   "source": [
    "# Este código deve ser executado em uma CÉLULA SEPARADA\n",
    "# após a célula que contém todas as definições de funções.\n",
    "\n",
    "print(\"Iniciando o processo de download e upload para o S3...\")\n",
    "\n",
    "# Para baixar todos os datasets definidos em DEFAULT_SPECS\n",
    "download_results = fetch_all(\n",
    "    datasets=list(DEFAULT_SPECS.keys()),\n",
    "    since=\"2018-01\",  # Exemplo: Baixar dados a partir de janeiro de 2024\n",
    "    overwrite=True    # Força o download mesmo se os arquivos já existirem\n",
    ")\n",
    "\n",
    "print(\"\\n--- Relatório Final de Processamento ---\")\n",
    "# Imprime o resultado do download para verificar quais arquivos foram salvos\n",
    "for key, s3_key_full in download_results.items():\n",
    "    if s3_key_full:\n",
    "        print(f\"✅ SUCESSO! Dataset '{key}' salvo em: s3://{S3_BUCKET}/{s3_key_full}\")\n",
    "    else:\n",
    "        print(f\"❌ FALHA! Dataset '{key}' não foi salvo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdb425-f0da-4d44-a0ea-12d745e6b846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43851900-9297-4036-8928-7721fb9d5c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
